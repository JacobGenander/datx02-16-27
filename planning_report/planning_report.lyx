#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{pgfgantt}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex8
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize a4paper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Planning report
\begin_inset Newline newline
\end_inset

Generating headlines with deep learning
\end_layout

\begin_layout Author
Alex Evert 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
and
\end_layout

\end_inset

 Jacob Genander 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
and
\end_layout

\end_inset

 Nicklas Lallo 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
and
\end_layout

\end_inset

 Rickard Lantz 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
and
\end_layout

\end_inset

 Filip Nilsson 
\end_layout

\begin_layout Date
2016-02-12
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% Defining this manually to avoid altering the style in the TOC
\end_layout

\begin_layout Plain Layout


\backslash
setlength{
\backslash
parskip}{
\backslash
medskipamount}
\end_layout

\begin_layout Plain Layout


\backslash
setlength{
\backslash
parindent}{0pt}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Text bodies coped from Google Docs 2016-02-04 13:40
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Section*
Abbreviations
\end_layout

\begin_layout Description
NN: Neural Network
\end_layout

\begin_layout Description
RNN: Recurrent Neural Network
\end_layout

\begin_layout Description
LSTM: Long Short-Term Memory
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Section*
Respons från Mikael
\end_layout

\begin_layout Plain Layout
Hej,
\end_layout

\begin_layout Plain Layout
Först måste jag säga att jag tycker den ser väldigt bra ut! fin struktur
 och det mesta är på plats.
\end_layout

\begin_layout Plain Layout
Bra jobbat.
\end_layout

\begin_layout Plain Layout
Nu lite kommentarer:
\end_layout

\begin_layout Itemize
första stycket "for the common programmer" Låter lite underligt.
 skulle stryka nog stryka det.
\end_layout

\begin_layout Itemize
Senare i första stycket står det "This is the science of artificial neural
 networks and it..." Här skulle det nog passa bättre med den mer allmänna termen
 Machine learning.
 Dock blir det tokigt om ni bara byter rakt av utan att anpassa då det påverkar
 stycket under.
\end_layout

\begin_layout Itemize
andra stycket "...better at high-level abstraction needed in" menar ni kanske
 hierarchical?
\end_layout

\begin_layout Itemize
tredje stycket "...stages we also get..." stryk also.
 Tips: alltid bra att titta igenom texten och se om det finns såna här ord
 man kan stryka för att göra budskapet tydligare.
\end_layout

\begin_layout Itemize
Ordningen på tasks.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Deciding hyperparameters är något man gör medan man tränar i en iterative
 process.
 
\end_layout

\begin_layout Itemize
Avoiding overfitting är en del i hyperparameter sökningen.
\end_layout

\end_deeper
\begin_layout Itemize
5.2.3: Lägg till information om vilka experiment ni vill köra för att evaluera
 era mål.
\end_layout

\begin_layout Itemize
6: Omöjligt att läsa erat gantt diag utskrivet på papper.
 Försök att göra det tydligare, eventuellt genom att ta bort onödiga detaljer.
\end_layout

\begin_layout Plain Layout
Annars var det som sagt bra.
 Bådar gott för framtiden!
\end_layout

\begin_layout Plain Layout
Mikael
\end_layout

\end_inset


\end_layout

\begin_layout Section
Background
\end_layout

\begin_layout Standard
Trying to create a computer program that recognizes speech or handwriting
 is a difficult task.
 A person’s voice is something highly individual and to find the common
 denominator between different people, a very sophisticated way of judgement
 is needed.
 In machine learning this has been tackled with artificial neural networks,
 taking inspiration from how the human brain works.
\end_layout

\begin_layout Standard
The idea is to make a computer learn a specific task or function by observing
 data.
 It can be mathematically shown that any function (within a compact set)
 can be modeled with a sufficiently big neural network and enough data to
 train on 
\begin_inset CommandInset citation
LatexCommand cite
key "kolmogorovs_theorem"

\end_inset

.
 The data is processed by interconnected “neurons” in different layers.
 We have an input layer, an output layer and layers in between these two.
 When the number of intermediate layers increase, we often refer to the
 network as a deep neural network.
 Generally deep nets are better at hierarchical abstraction needed in, for
 example, image recognition.
\end_layout

\begin_layout Standard
By making it possible for the neurons to loop back information to earlier
 stages we get something similar to memory.
 These kinds of neural networks where information can persist is known as
 recurrent neural networks (RNNs).
 The ability to remember things makes these networks especially interesting
 in language modeling where we might try to generate sentences based on
 previous words the network has seen.
\end_layout

\begin_layout Standard
Even though ordinary recurrent networks can remember their state, they are
 often bad at long term dependencies.
 In these kind of situations a special incarnation of RNN is used called
 Long Short Term Memory or simply LSTM.
 Basically an LSTM network contains read, write and reset operations that
 makes it possible for the network to learn when to discard, modify or keep
 information.
 This makes them excellent when information needs to be remembered over
 long periods of time.
 
\end_layout

\begin_layout Section
Purpose
\end_layout

\begin_layout Standard
This project is about developing an artificial neural network of LSTM type
 that can learn to generate common news headlines, based on some input text.
 We will also explore the possibilities to implement an attention model
 so that the network can choose, on its own, which parts of an article that
 are most relevant for its conditioning.
 Hopefully the project will shed some light over LSTM-networks use in language
 modelling and inspire to further studies in machine learning.
\end_layout

\begin_layout Section
Task
\end_layout

\begin_layout Standard
Our goal is to construct a neural network capable of generating titles for
 articles.
 This is quite a complex task which can be broken down into the following
 sub-tasks:
\end_layout

\begin_layout Subsection
Collecting data
\end_layout

\begin_layout Standard
To acquire good results, it is essential that the network has a high-quality
 and extensive data-set to learn from.
 It is also preferable to have data which can be used to validate the performanc
e of the network (such data should be separate from the training data).
\end_layout

\begin_layout Standard
In our case, the primary source of data is articles and their titles.
 Assembling such a data-set can be done either manually or automatically
 by scraping websites, or more preferably by utilizing already existing
 article databases.
\end_layout

\begin_layout Subsection
Processing the data
\end_layout

\begin_layout Standard
The articles must be prepared before they can be used in training the network.
 One purpose of the processing is to reduce the inherent complexity of written
 text into something more manageable for the neural network.
 Data processed in different ways can also change which patterns are recognized
 by the network.
\end_layout

\begin_layout Subsection
Training the network
\end_layout

\begin_layout Standard
The process in which the internal parameters of the network are decided
 to make it generate the best results is commonly known as training.
 By comparing generated results with real life answers we get an error (or
 cost) that we want to minimize.
 This can be done in several ways, both numerical and exact.
 The best choice depends on the specific task and the number of features
 (inputs).
\end_layout

\begin_layout Standard
Some parameters cannot be learned, but has to be set manually.
 These so called hyper-parameters are mainly related to the specific learning
 algorithm and structure of the network.
 Essentially, the structural parameters are the number of layers in the
 network and how many neurons they contain.
 For the training process we also need to decide the number of iterations,
 learning rate and what type of optimization method that is going to be
 used.
\end_layout

\begin_layout Standard
A trained network should be able to generalize what it has learnt to new
 cases it has never seen before.
 Sometimes this can not be done if the new data differ too much from the
 training set.
 This is the problem of overfitting and might arise when we try to approximate
 dependencies in a training set too closely.
 The decision of hyper parameters is closely related to this, but there
 are several other ways to get better approximations.
 One is to simply increase the amount of training data, another is to use
 the process of regularization where we include some additional information
 in the optimization step to give the model more knowledge about the data.
 Two popular such methods are “Early stopping” - where a validation set
 is used to determine when to stop the training - and “Dropout” - where
 only a fraction of the neurons are simultaneously active during training.
 
\end_layout

\begin_layout Subsection
Attention model
\end_layout

\begin_layout Standard
Apart from setting up the initial deep neural network for the generation
 of headlines, we will attempt, if time allows, to make the network take
 some parts of the article text into consideration more than others.
 This would be done using a so called attention model, which focuses on
 relevant parts of a text.
 Doing so, the network could possibly generate more accurate headlines,
 since words with less importance with respect to the content and intention
 of the article are omitted.
 This would be especially useful when dealing with longer text 
\begin_inset CommandInset citation
LatexCommand cite
key "bahdanau2014neural"

\end_inset

.
\end_layout

\begin_layout Section
Scope
\end_layout

\begin_layout Standard
Networks of the LSTM-type involve various mathematical calculations, both
 during the training phase and generation of headlines.
 However, there seems to be a lack of mathematical proofs that LSTM-networks
 and artificial neural networks in general always works as intended.
 As mentioned in the background section, many breakthroughs have been made
 in recent years using LSTM-networks.
 Therefore, the project will not involve any attempts for such a proof,
 but rather rely on the fact that the technique is evidently working great
 for tasks like this 
\begin_inset CommandInset citation
LatexCommand cite
key "sequences_rnn"

\end_inset

.
\end_layout

\begin_layout Standard
The project will only involve training the network on English text, since
 adding an additional language would make the task more complex.
 Learning multiple languages, the training time of the network would probably
 be longer as well.
\end_layout

\begin_layout Standard
Further, the network will only be trained on written text.
 More specifically, the network will only be trained to generate headlines
 for articles, not for books or other written media.
 We aim to train the network to recognize the specific style of headlines.
 To mix this style with e.g.
 book titles could lead to headlines that do not seem natural to find among
 articles.
 
\end_layout

\begin_layout Standard
The network approximates an unknown conditional distribution that changes
 dramatically over time, i.e.
 the style of newspaper articles from from the 19th century may be very
 different from those written in the past 10 years.
 Articles stemming from different sources may also be very different in
 style.
 This could potentially be a problem when measuring how good the network
 generates an appropriate headline for a given article.
 Therefore, we will either train a suite of networks and compare the results,
 or narrow down the timespan for publication dates of the articles and use
 a concise source of articles, e.g.
 from one newspaper.
\end_layout

\begin_layout Section
Method
\end_layout

\begin_layout Standard
The following paragraphs explain the working process of the project.
 It mentions what data will be obtained, how it will be processed and what
 tools may be used for constructing the network and to evaluate its ability
 to generate headlines
\end_layout

\begin_layout Subsection
Data
\end_layout

\begin_layout Standard
We intend to use existing datasets, and have identified The Corpus of Contempora
ry American English (COCA), Wikinews, Yahoo News and Google News as corpuses
 to consider.
\end_layout

\begin_layout Subsubsection
Processing
\end_layout

\begin_layout Standard
Before the text corpus will be given to the network we will use a word embedding
 like word2vec 
\begin_inset CommandInset citation
LatexCommand cite
key "word2vec"

\end_inset

 or Glove 
\begin_inset CommandInset citation
LatexCommand cite
key "glove"

\end_inset

 to create vector representations of the words.
 This representation has the advantage of placing related words close to
 each other in vector space 
\begin_inset CommandInset citation
LatexCommand cite
key "eff_est_of_words_in_vector_space"

\end_inset

.
 When generating headlines, a vector representation will give us some error
 margin since a wrongly predicted word hopefully is close to the relevant
 context.
\end_layout

\begin_layout Subsection
Network construction and evaluation
\end_layout

\begin_layout Standard
The network construction will be done with a high level of abstraction ---
 instead of implementing algorithms etc.
 from scratch using the mathematical definitions, we intend to use a framework
 called TensorFlow 
\begin_inset CommandInset citation
LatexCommand cite
key "tensorflow"

\end_inset

.
 Python will be used as the primary programming language, as it is the default
 language in the API.
 The source-code will be version controlled with Git and hosted on GitHub.
\end_layout

\begin_layout Subsubsection
TensorFlow
\end_layout

\begin_layout Standard
TensorFlow is a framework by Google which provides many functions frequently
 used when working with machine learning.
 We will use many of its methods for constructing and training neural networks.
 In addition, TensorFlow has native support for multi-threaded computation
 on both CPUs and GPUs, which allows for computational speedups
\end_layout

\begin_layout Subsubsection
Network training
\end_layout

\begin_layout Standard
The training process, the validation process, and the tuning of parameters
 will be a somewhat iterative process.
 Experiments with certain parameters will be recorded, and the parameters
 optimized in steps.
 In addition to our personal computers, we have been given access to a powerful
 GPU-equipped workstation.
 Since the training process can be computationally expensive and time-consuming,
 training sessions with larger datasets must be planned and parameters evaluated
 before using this resource.
\end_layout

\begin_layout Subsubsection
Performance evaluation
\end_layout

\begin_layout Standard
For every major design decision we will perform the same experiment on a
 sample of the test set.
 
\end_layout

\begin_layout Standard
Evaluation of our results will be done using a Turing-style test where subjects
 will be shown news articles with either generated or real headlines and
 asked to pick out the articles with generated headlines.
 We will also evaluate the form of the headlines in a similar manner but
 without making the subjects read the article but simply try to pick out
 machine generated headlines.
 
\end_layout

\begin_layout Standard
Apart from the manual evaluation we will also use the Recall-Oriented Understudy
 for Gisting Evaluation, ROUGE 
\begin_inset CommandInset citation
LatexCommand cite
key "lin2004rouge"

\end_inset

, framework developed by Yin-Yew Lin to automatically score our generated
 headlines.
\end_layout

\begin_layout Section
Time plan
\end_layout

\begin_layout Standard
The initial time plan can be seen in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:Gantt-chart"

\end_inset

, which may be due to changes.
 The dates are presented as a Gantt chart.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways true
status open

\begin_layout Plain Layout
\begin_inset CommandInset include
LatexCommand include
filename "pgfgantt/gantt_chart.tex"

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Gantt chart of the preliminary time plan.
 Deadlines are marked with diamonds
\begin_inset CommandInset label
LatexCommand label
name "fig:Gantt-chart"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "../references"
options "bibtotoc,IEEEtran"

\end_inset


\end_layout

\end_body
\end_document
